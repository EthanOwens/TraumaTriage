{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import norm\n",
    "from functools import reduce\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHavkW5rZzqM"
   },
   "source": [
    "### Consolidated Tree Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1060,
     "status": "ok",
     "timestamp": 1638626158593,
     "user": {
      "displayName": "abhi arjun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18055772853659977353"
     },
     "user_tz": -330
    },
    "id": "rccj865bcAzV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def gini_index(y):\n",
    "    N = len(y)\n",
    "    s1 = (y == 1).sum()\n",
    "    #If all or none are of the same class, return infinite value\n",
    "    if 0 == s1 or N == s1:\n",
    "        return np.Inf\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    classes_prob = [counts[i]/np.sum(counts) for i in range(len(counts))] #fraction of \n",
    "    #Gini Formula: 1- SUMMATION(p^2)\n",
    "    return np.round(1 - np.sum(np.square(classes_prob)),4)\n",
    "\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, depth=1, max_depth=None, min_samples_split = 5):\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.impurity = None\n",
    "        if self.max_depth is not None and self.max_depth < self.depth:\n",
    "            raise Exception(\"depth > max_depth\")\n",
    "        self.min_samples_split = min_samples_split\n",
    "        \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        if (len(Y) < self.min_samples_split or len(set(Y)) == 1):\n",
    "            self.col = None\n",
    "            self.split = None\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.prediction = Y[0]\n",
    "\n",
    "        else:\n",
    "            #Could be written as: cols = range(X.shape[1])\n",
    "            D = X.shape[1]\n",
    "            cols = range(D)\n",
    "            \n",
    "            \n",
    "            min_imp = np.Inf #min_imp = Min impurity\n",
    "            best_col = None\n",
    "            best_split = None\n",
    "            # Finding First Split\n",
    "            #Find the feature (X col) with the highest possible purity \n",
    "            for col in cols:\n",
    "                imp, split = self.find_split(X, Y, col)\n",
    "                if imp < min_imp:\n",
    "                    min_imp = imp\n",
    "                    best_col = col\n",
    "                    best_split = split\n",
    "\n",
    "            #If the most pure split is all of one class then don't split\n",
    "            if min_imp == np.Inf:\n",
    "                self.col = None\n",
    "                self.split = None\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.prediction = np.round(Y.mean())\n",
    "                return\n",
    "\n",
    "            self.col = best_col\n",
    "            self.split = best_split\n",
    "            self.impurity = min_imp\n",
    "            #if defined max depth is reached, don't make leaf nodes\n",
    "            if (self.depth == self.max_depth) or (D == 0):\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.prediction = [\n",
    "                    np.round(Y[X[:,best_col] < self.split].mean()),\n",
    "                    np.round(Y[X[:,best_col] >= self.split].mean()),\n",
    "                ]\n",
    "            \n",
    "            #Split up tuples by the split rule and recursively make new nodes until 0 gini index (all nodes are pure)\n",
    "            else:\n",
    "                left_idx = (X[:,best_col] < best_split)\n",
    "                # print(left_idx)\n",
    "                Xleft = X[left_idx]\n",
    "                Yleft = Y[left_idx]\n",
    "                self.left = TreeNode(self.depth + 1, self.max_depth)\n",
    "                self.left.fit(Xleft, Yleft)\n",
    "                self.left.impurity = gini_index(Yleft)\n",
    "\n",
    "                right_idx = (X[:,best_col] >= best_split)\n",
    "                Xright = X[right_idx]\n",
    "                Yright = Y[right_idx]\n",
    "                self.right = TreeNode(self.depth + 1, self.max_depth)\n",
    "                self.right.fit(Xright, Yright)\n",
    "                self.right.impurity = gini_index(Yright)\n",
    "        \n",
    "  \n",
    "        \n",
    "        \n",
    "                \n",
    "    #Sort X and Y by sorted(X) indexes. Split rule made on X mean. \n",
    "    def find_split(self, X, Y, col):\n",
    "        x_values = X[:, col] #X Values in the column\n",
    "        sort_idx = np.argsort(x_values)\n",
    "        #Sort X and Y index by the sorted X values\n",
    "        x_values = x_values[sort_idx]\n",
    "        y_values = Y[sort_idx]\n",
    "        # Split rule based on mean\n",
    "        split = np.mean(x_values) \n",
    "        imp = self.gini_impurity(x_values, y_values, split)\n",
    "        return imp, split\n",
    "\n",
    "    #Returns impurity level of the split \n",
    "    def gini_impurity(self, x, y, split):\n",
    "        y0 = y[x < split]\n",
    "        y1 = y[x >= split]\n",
    "        n_samples_0, n_samples_1 = len(y0), len(y1)\n",
    "        gini_y0, gini_y1 = gini_index(y0), gini_index(y1)\n",
    "        # print(n_samples_0, n_samples_1)\n",
    "        split_impurity = (n_samples_0*(gini_y0) + n_samples_1*(gini_y1))/(n_samples_0 + n_samples_1)\n",
    "        return split_impurity\n",
    "\n",
    "    #!! Too much nesting. Will try to break it up when refactoring so it's more readable\n",
    "    def predict_one(self, x): \n",
    "        # use \"is not None\" because 0 means False\n",
    "        if self.col is not None and self.split is not None:\n",
    "            feature = x[self.col]\n",
    "            if feature < self.split:\n",
    "                if self.left:\n",
    "                    p = self.left.predict_one(x)\n",
    "                else:\n",
    "                    p = self.prediction[0]\n",
    "            else:\n",
    "                if self.right:\n",
    "                    p = self.right.predict_one(x)\n",
    "                else:\n",
    "                    p = self.prediction[1]\n",
    "        else:\n",
    "            # corresponds to having only 1 prediction\n",
    "            p = self.prediction\n",
    "        return p\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        N = len(X)\n",
    "        P = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            P[i] = self.predict_one(X[i])\n",
    "        return P\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_sample_split = 5):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.root = TreeNode(max_depth=self.max_depth)\n",
    "        self.root.fit(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.root.predict(X)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        P = self.predict(X)\n",
    "        return np.mean(P == Y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1060,
     "status": "ok",
     "timestamp": 1638626158593,
     "user": {
      "displayName": "abhi arjun",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "18055772853659977353"
     },
     "user_tz": -330
    },
    "id": "rccj865bcAzV"
   },
   "outputs": [],
   "source": [
    "#uninformative names, change when refactoring. Need to ask Sharania about them \n",
    "l1=[]\n",
    "l3=[]\n",
    "l=[]\n",
    "l2 = []\n",
    "counter = 0\n",
    "leafnodes = []\n",
    "predict_leafnode = []\n",
    "class CTCNode:\n",
    "    def __init__(self, depth=1, max_depth=None, min_samples_split = 5):\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        if self.max_depth is not None and self.max_depth < self.depth:\n",
    "            raise Exception(\"depth > max_depth\")\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.mean_ = None \n",
    "        self.std_ = None\n",
    "        self.gauss_prob = None\n",
    "        self.node_type = None\n",
    "        cumu_prob_right = None\n",
    "        Cumu_prob_left = None\n",
    "\n",
    "\n",
    "    def subsample(self, X, Y, n_samples = 10, split_percent = 0.7):\n",
    "        if len(set(Y)) == 1:\n",
    "            return \n",
    "        Trees = [DecisionTree(max_depth = 1) for i in range(n_samples)]\n",
    "        Votes = []\n",
    "        SplitValues = []\n",
    "        index = []\n",
    "        sample = {}\n",
    "        Counter = 0\n",
    "        #Make n_samples sub trees (bootstrapping method), then fit each one \n",
    "        for Tree in Trees:\n",
    "            idxs = np.random.choice(X.shape[0], size=int(X.shape[0]*split_percent), replace=True) # Random Sampling \n",
    "            Xb = X[idxs]\n",
    "            Yb = Y[idxs]\n",
    "            if len(set(Yb)) == 1:\n",
    "                continue\n",
    "            Tree.fit(Xb, Yb)\n",
    "            index.append(idxs)\n",
    "            key = Tree\n",
    "            value = idxs\n",
    "            sample.update({key:value})\n",
    "            Votes.append(Tree.root.col)\n",
    "            SplitValues.append(Tree.root.split)\n",
    "        index = list(np.array(index))\n",
    "        Votes = np.array(Votes)\n",
    "        SplitValues = np.array(SplitValues)\n",
    "        \n",
    "        #get votes for most important column, prunes empty votes if error is caught\n",
    "        try:\n",
    "            cols, counts = np.unique(Votes, return_counts=True)\n",
    "        except:\n",
    "            idxs = np.where(Votes != None)\n",
    "            Votes = Votes[idxs]\n",
    "            SplitValues = SplitValues[idxs]\n",
    "            cols, counts = np.unique(Votes, return_counts=True)\n",
    "        if len(counts) == 0:\n",
    "            return\n",
    "        VoteWinner = cols[np.argmax(counts)]\n",
    "        self.best_col = VoteWinner\n",
    "        Splits = []\n",
    "        #Find the split rule of each tree\n",
    "        for idx,Tree in enumerate(Trees):\n",
    "            if idx >= len(index):\n",
    "                break\n",
    "            idxs = index[idx]\n",
    "            #print(idxs)\n",
    "            Xa = X[idxs]\n",
    "            Ya = Y[idxs]\n",
    "            x_values = Xa[:, VoteWinner]\n",
    "            sort_idx = np.argsort(x_values)\n",
    "            x_values = x_values[sort_idx]\n",
    "            y_values = Ya[sort_idx]\n",
    "            # Splitting based on mean\n",
    "            split = np.mean(x_values)\n",
    "            Splits.append(split)\n",
    "        Splits = np.array(Splits)    \n",
    "        self.col = VoteWinner\n",
    "        self.mean_ = np.mean(Splits)\n",
    "        self.std_ = np.std(Splits)\n",
    "        self.gauss_prob= norm(loc = self.mean_,scale= self.std_).cdf(Splits[0])\n",
    "    \n",
    "\n",
    "\n",
    "    def leaf(self, Y):\n",
    "        self.col = None\n",
    "        self.split = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        leafnodes.append(reduce((lambda x, y: x * y), l)) #idk what \"l\" is\n",
    "        if (len(l2) > 0 ):\n",
    "            leafnodes.append(reduce((lambda x, y: x * y), l2))\n",
    "        self.prediction = Y[0]\n",
    "        \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        if (len(Y) < self.min_samples_split or len(set(Y)) == 1):\n",
    "            self.leaf(Y)\n",
    "            return\n",
    "        else:\n",
    "            D = X.shape[1]\n",
    "            self.subsample(X, Y)\n",
    "            #Means node is pure and becomes a leaf\n",
    "            if (self.mean_ is None or self.std_ is None or self.col is None):\n",
    "                self.leaf(Y)\n",
    "                return\n",
    "            #max depth was reached so no new nodes are made\n",
    "            if (self.depth == self.max_depth) or (D == 0):\n",
    "                self.left = None\n",
    "                self.right = None\n",
    "                self.prediction = [\n",
    "                    np.round(Y[X[:,self.col] < self.mean_].mean()),\n",
    "                    np.round(Y[X[:,self.col] >= self.mean_].mean()),\n",
    "                ]\n",
    "            #only works with binary split, should work with categorical as well\n",
    "            else:\n",
    "                l.append(self.gauss_prob)\n",
    "                left_idx = (X[:,self.col] < self.mean_)\n",
    "                Xleft = X[left_idx]\n",
    "                Yleft = Y[left_idx]\n",
    "\n",
    "                self.left = CTCNode(self.depth + 1, self.max_depth)\n",
    "                self.left.fit(Xleft, Yleft)\n",
    "                \n",
    "                \n",
    "\n",
    "                l2.append(1-self.gauss_prob)\n",
    "                #print(l2)\n",
    "                right_idx = (X[:,self.col] >= self.mean_)\n",
    "                Xright = X[right_idx]\n",
    "                Yright = Y[right_idx]\n",
    "                self.right = CTCNode(self.depth + 1, self.max_depth)\n",
    "                self.right.fit(Xright, Yright)\n",
    "                \n",
    "\n",
    "   \n",
    "    #Has no return for most likely circumstance\n",
    "    def predict_one(self, x):\n",
    "        # use \"is not None\" because 0 means False\n",
    "        if self.col is not None and self.mean_ is not None:\n",
    "            feature = x[self.col]\n",
    "            #recursively move down the tree until you find the leaf\n",
    "            if feature < self.mean_:\n",
    "                if self.left: \n",
    "                    l1.append(self.gauss_prob)\n",
    "                    cumu_gauss_left = reduce((lambda x, y: x * y), l1)\n",
    "                    p = self.left.predict_one(x)\n",
    "                if self.leaf:\n",
    "\n",
    "                    predict_leafnode.append(cumu_gauss_left)\n",
    "                    \n",
    "                else:\n",
    "                    p = self.prediction[0]\n",
    "                    \n",
    "            else:\n",
    "                if self.right:\n",
    "                    l3.append(1-self.gauss_prob)\n",
    "                    cumu_gauss_right = reduce((lambda x, y: x * y), l3)\n",
    "                    p = self.right.predict_one(x)\n",
    "                if self.leaf:\n",
    "                    predict_leafnode.append(cumu_gauss_right)\n",
    "                else:\n",
    "                    p = self.prediction[1]\n",
    "           \n",
    "        else:\n",
    "            # corresponds to having only 1 prediction\n",
    "            p = self.prediction   \n",
    "        return p\n",
    "\n",
    "    def predict(self, X):\n",
    "        N = len(X)\n",
    "        P = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            P[i] = self.predict_one(X[i])\n",
    "        return P\n",
    "\n",
    "    \n",
    "     \n",
    "\n",
    "class CTC_DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_sample_split = 5):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.root = CTCNode(max_depth=self.max_depth)\n",
    "        self.root.fit(X, Y)\n",
    "       \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.root.predict(X)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        P = self.predict(X)\n",
    "        return np.mean(P == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running just like scikit learn\n",
    "# model = CTC_DecisionTree(max_depth=13)\n",
    "# model.fit(Xtrain, Ytrain)\n",
    "\n",
    "#y_pred = model.predict(Xtest)\n",
    "\n",
    "# model.score(Xtest,Ytest) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/ta69ofB1TVcUYWyq2w4A",
   "collapsed_sections": [
    "v9Ah5KEzWzx_",
    "wRtTt-5uXM_X"
   ],
   "mount_file_id": "1bAGH9Q3U-jcUWkai2Uqs1X4agxoFXONv",
   "name": "FinalAlgo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
